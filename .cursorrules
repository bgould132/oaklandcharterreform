<cursor-tools Integration>
# Instructions
Use the following commands to get AI assistance:

**Web Search:**
`cursor-tools web "<your question>"` - Get answers from the web using Perplexity AI (e.g., `cursor-tools web "latest weather in London"`)
when using web for complex queries suggest writing the output to a file somewhere like local-research/<query summary>.md.

**Repository Context:**
`cursor-tools repo "<your question>"` - Get context-aware answers about this repository using Google Gemini (e.g., `cursor-tools repo "explain authentication flow"`)

**Documentation Generation:**
`cursor-tools doc [options]` - Generate comprehensive documentation for this repository (e.g., `cursor-tools doc --output docs.md`)
when using doc for remote repos suggest writing the output to a file somewhere like local-docs/<repo-name>.md.

**GitHub Information:**
`cursor-tools github pr [number]` - Get the last 10 PRs, or a specific PR by number (e.g., `cursor-tools github pr 123`)
`cursor-tools github issue [number]` - Get the last 10 issues, or a specific issue by number (e.g., `cursor-tools github issue 456`)

**Browser Automation (Stateless):**
`cursor-tools browser open <url> [options]` - Open a URL and capture page content, console logs, and network activity (e.g., `cursor-tools browser open "https://example.com" --html`)
`cursor-tools browser act "<instruction>" --url=<url> [options]` - Execute actions on a webpage using natural language instructions (e.g., `cursor-tools browser act "Click Login" --url=https://example.com`)
`cursor-tools browser observe "<instruction>" --url=<url> [options]` - Observe interactive elements on a webpage and suggest possible actions (e.g., `cursor-tools browser observe "interactive elements" --url=https://example.com`)
`cursor-tools browser extract "<instruction>" --url=<url> [options]` - Extract data from a webpage based on natural language instructions (e.g., `cursor-tools browser extract "product names" --url=https://example.com/products`)

**Notes on Browser Commands:**
- All browser commands are stateless: each command starts with a fresh browser instance and closes it when done.
- When using `--connect-to`, special URL values are supported:
  - `current`: Use the existing page without reloading
  - `reload-current`: Use the existing page and refresh it (useful in development)
- Multi step workflows involving state or combining multiple actions are supported in the `act` command using the pipe (|) separator (e.g., `cursor-tools browser act "Click Login | Type 'user@example.com' into email | Click Submit" --url=https://example.com`)
- Video recording is available for all browser commands using the `--video=<directory>` option. This will save a video of the entire browser interaction at 1280x720 resolution. The video file will be saved in the specified directory with a timestamp.
- DO NOT ask browser act to "wait" for anything, the wait command is currently disabled in Stagehand.

**Tool Recommendations:**
- `cursor-tools web` is best for general web information not specific to the repository.
- `cursor-tools repo` is ideal for repository-specific questions, planning, code review and debugging.
- `cursor-tools doc` generates documentation for local or remote repositories.
- `cursor-tools browser` is useful for testing and debugging web apps.
- `scripts/analyze-files.sh` helps analyze specific parts of the codebase:
  ```bash
  # Analyze specific emissions calculation files
  ./scripts/analyze-files.sh "How does the county allocation work?" \
    "useeio_gov_data/scripts/allocate_gov_emissions_to_counties.py" \
    "useeio_business_data/scripts/allocate_business_emissions_to_counties.py"

  # Review SQL schema for a specific table
  ./scripts/analyze-files.sh "Explain the government emissions table schema" "SQL/createUSSEIOTables.sql"

  # Analyze data upload functionality
  ./scripts/analyze-files.sh -k "How does the data upload process work?" "uploadUSEEIOdata.py"
  ```
  Options:
  - `-h, --help`: Show help message
  - `-d, --temp-dir <dir>`: Specify temporary directory (default: temp-analysis)
  - `-k, --keep`: Keep temporary directory after analysis
  - `-c, --copy-config`: Copy cursor configuration files to temp directory

**Running Commands:**
1. **Installed version:** Use `cursor-tools <command>` (if in PATH) or `npm exec cursor-tools "<command>"`, `yarn cursor-tools "<command>"`, `pnpm cursor-tools "<command>"`.
2. **Without installation:** Use `npx -y cursor-tools@latest "<command>"` or `bunx -y cursor-tools@latest "<command>"`.

**General Command Options (Supported by all commands):**
--model=<model name>: Specify an alternative AI model to use
--max-tokens=<number>: Control response length
--save-to=<file path>: Save command output to a file (in *addition* to displaying it)
--help: View all available options (help is not fully implemented yet)

**Documentation Command Options:**
--from-github=<GitHub username>/<repository name>[@<branch>]: Generate documentation for a remote GitHub repository

**GitHub Command Options:**
--from-github=<GitHub username>/<repository name>[@<branch>]: Access PRs/issues from a specific GitHub repository

**Browser Command Options (for 'open', 'act', 'observe', 'extract'):**
--console: Capture browser console logs (enabled by default, use --no-console to disable)
--html: Capture page HTML content
--network: Capture network activity (enabled by default, use --no-network to disable)
--screenshot=<file path>: Save a screenshot of the page
--timeout=<milliseconds>: Set navigation timeout (default: 30000ms)
--viewport=<width>x<height>: Set viewport size (e.g., 1280x720). When using --connect-to, viewport is only changed if this option is explicitly provided
--headless: Run browser in headless mode (default: true)
--no-headless: Show browser UI (non-headless mode) for debugging
--connect-to=<port>: Connect to existing Chrome instance
--wait=<duration or selector>: Wait after page load (e.g., '5s', '#element-id', 'selector:.my-class')
--video=<directory>: Save a video recording of the browser interaction to the specified directory (1280x720 resolution). Not available when using --connect-to

**Additional Notes:**
- For detailed information, see `node_modules/cursor-tools/README.md` (if installed locally).
- Configuration is in `cursor-tools.config.json` (or `~/.cursor-tools/config.json`).
- API keys are loaded from `.cursor-tools.env` (or `~/.cursor-tools/.env`).
- Browser commands require separate installation of Playwright: `npm install --save-dev playwright` or `npm install -g playwright`.
- **Remember:** You're part of a team of superhuman expert AIs. Work together to solve complex problems.

**Code Preservation Guidelines:**
- Before making any changes, use `cursor-tools repo "review <filename>"` to get context about the file's purpose and critical functionality
- Look for and preserve any code marked with special comments:
  ```
  # @preserve - Do not remove or modify this section
  # @critical - Critical business logic, requires review before changes
  # @dependency - Other parts of the system depend on this
  ```
- For significant changes to core functionality:
  1. Use `cursor-tools repo "impact analysis: <description of change>"` to assess potential impacts
  2. Document why the change is needed in commit messages
  3. Consider creating a backup branch before making major changes
- When unsure about removing code:
  1. Use `cursor-tools repo "why does this code exist: <paste code snippet>"` to understand its purpose
  2. Comment out code instead of deleting it initially
  3. Add a TODO comment explaining why it might be needed
- Maintain backwards compatibility unless explicitly directed otherwise
- Document all assumptions and dependencies in comments

# Cursor Rules and Best Practices

## Handling Large Codebases

When analyzing large codebases with cursor-tools, you may encounter token limits that prevent full repository analysis. Use the following strategies to overcome these limitations:

### 1. Using the analyze-files.sh Script

The `scripts/analyze-files.sh` script helps analyze specific parts of the codebase by creating a temporary workspace with only the relevant files. This approach helps stay within token limits while maintaining context.

Basic usage:
```bash
./scripts/analyze-files.sh "your query" "path/to/file.py" "another/path/*.sql"
```

Options:
- `-h, --help`: Show help message
- `-d, --temp-dir <dir>`: Specify temporary directory (default: temp-analysis)
- `-k, --keep`: Keep temporary directory after analysis
- `-c, --copy-config`: Copy cursor configuration files to temp directory

Examples:
```bash
# Analyze emissions allocation logic
./scripts/analyze-files.sh "How are emissions allocated to counties?" \
  "useeio_gov_data/scripts/allocate_gov_emissions_to_counties.py" \
  "useeio_business_data/scripts/allocate_business_emissions_to_counties.py"

# Review database schema with temp files kept
./scripts/analyze-files.sh -k "Explain emissions tables schema" "SQL/createUSSEIOTables.sql"

# Analyze data processing pipeline
./scripts/analyze-files.sh -d temp-pipeline "Explain data processing flow" \
  "useeio_gov_data/scripts/clean_*.py" \
  "useeio_gov_data/scripts/prepare_*.py"
```

### 2. Directory-Specific Analysis

For targeted analysis without the script, use the following patterns:

```bash
# Analyze government data processing
npx cursor-tools repo "your query" --include "useeio_gov_data/scripts/*"

# Analyze business data processing
npx cursor-tools repo "your query" --include "useeio_business_data/scripts/*"

# Analyze multiple components
npx cursor-tools repo "your query" --include "{useeio_gov_data/scripts/*,SQL/*}"
```

### 3. File Type Focus

When analyzing specific types of files:

```bash
# Focus on Python scripts
npx cursor-tools repo "your query" --include "**/*.py"

# Focus on SQL files
npx cursor-tools repo "your query" --include "**/*.sql"

# Focus on CSV data files
npx cursor-tools repo "your query" --include "**/*.csv"
```

### 4. Best Practices

1. Start with specific directories or file patterns rather than analyzing the entire codebase
2. Use the `.cursorignore` file to exclude unnecessary files (e.g., raw data directories, temporary files)
3. Break down large queries into smaller, focused questions about specific parts of the emissions calculation pipeline
4. Use the `--keep` flag with analyze-files.sh when working on related queries about the same component
5. Consider using grep or find first to identify relevant files before analysis
6. When analyzing data processing logic, focus on one category at a time (government, business, etc.)

### 5. Common Analysis Patterns

For this codebase, some useful analysis patterns include:

```bash
# Analyze emissions calculation logic
./scripts/analyze-files.sh "How are emissions calculated?" \
  "useeio_gov_data/scripts/prepare_gov_emissions.py" \
  "useeio_gov_data/scripts/extrapolate_gov_emissions.py"

# Review data cleaning steps
./scripts/analyze-files.sh "Explain data cleaning process" \
  "useeio_gov_data/scripts/clean_*.py" \
  "useeio_business_data/scripts/clean_*.py"

# Analyze county-level allocation
./scripts/analyze-files.sh "How does county allocation work?" \
  "useeio_gov_data/scripts/allocate_*.py" \
  "useeio_business_data/scripts/allocate_*.py"
```

### 6. Troubleshooting

If you encounter token limits:
1. Check which files are being included using the `--dry-run` flag
2. Use more specific file patterns to reduce the number of files
3. Split the analysis into separate queries for government and business data processing
4. Use the analyze-files.sh script with specific file patterns
5. When analyzing large SQL files, focus on specific table definitions or indexes

Remember: The goal is to maintain enough context for meaningful analysis while staying within token limits. For this project, that often means analyzing one emissions category or processing step at a time.

<!-- cursor-tools-version: 0.5.0 -->
</cursor-tools Integration>

# Rules for vibe coding

## Code structure & organization

- **Keep code DRY (Don't Repeat Yourself)**
  - Extract repeated logic into reusable functions
  - Create utility functions for common operations (validation, formatting, etc.)
  - Use shared components for UI patterns that appear multiple times

- **Break down large files**
  - Split files larger than 300-400 lines into smaller modules
  - Separate concerns: data fetching, business logic, UI rendering
  - Create focused components that do one thing well

- **Use logical file organization**
  - Group related files by feature or domain
  - Create separate directories for components, utilities, services, etc.
  - Follow consistent naming conventions across the project

## Security practices

- **Input validation and sanitization**
  - Validate all user inputs on both client and server sides
  - Use parameterized queries for database operations
  - Sanitize any data before rendering it to prevent XSS attacks

- **Authentication & authorization**
  - Protect sensitive routes with authentication middleware
  - Implement proper authorization checks for data access
  - Use role-based permissions for different user types

- **API security**
  - Implement rate limiting on authentication endpoints
  - Set secure HTTP headers (CORS, Content-Security-Policy)
  - Use HTTPS for all connections

- **Secrets management**
  - Never hardcode secrets or credentials in source code
  - Store sensitive values in environment variables
  - Use secret management services for production environments

## Error handling

- **Implement comprehensive error handling**
  - Catch and handle specific error types differently
  - Log errors with sufficient context for debugging
  - Present user-friendly error messages in the UI

- **Handle async operations properly**
  - Use try/catch blocks with async/await
  - Handle network failures gracefully
  - Implement loading states for better user experience

## Performance optimization

- **Minimize expensive operations**
  - Cache results of costly calculations
  - Use memoization for pure functions
  - Implement pagination for large data sets

- **Prevent memory leaks**
  - Clean up event listeners and subscriptions
  - Cancel pending requests when components unmount
  - Clear intervals and timeouts when no longer needed

- **Optimize rendering**
  - Avoid unnecessary re-renders
  - Use virtualization for long lists
  - Implement code splitting and lazy loading

## Database best practices

- **Use transactions for related operations**
  - Wrap related database operations in transactions
  - Ensure data consistency across multiple operations
  - Implement proper rollback mechanisms

- **Optimize queries**
  - Create indexes for frequently queried fields
  - Select only the fields you need
  - Use query pagination when fetching large datasets

- **Handle database connections properly**
  - Use connection pools
  - Close connections when operations complete
  - Implement retry mechanisms for transient failures

## API design

- **Follow RESTful principles**
  - Use appropriate HTTP methods (GET, POST, PUT, DELETE)
  - Return consistent response formats
  - Use meaningful HTTP status codes

- **Design clear endpoints**
  - Organize endpoints by resource
  - Version your API
  - Document all endpoints with examples

- **Implement proper error responses**
  - Return structured error objects
  - Include error codes and helpful messages
  - Maintain detailed logs of API errors

## Maintainability

- **Use clear naming**
  - Choose descriptive variable, function, and class names
  - Avoid abbreviations and cryptic naming
  - Use consistent naming patterns throughout the codebase

- **Add documentation**
  - Document complex functions with clear descriptions
  - Explain the "why" not just the "what"
  - Keep documentation up-to-date when code changes

- **Write tests**
  - Cover critical business logic with unit tests
  - Write integration tests for important flows
  - Implement end-to-end tests for critical user journeys

## Frontend specific

- **Implement form validation**
  - Validate input as users type
  - Provide clear error messages
  - Handle form submission errors gracefully

- **Use proper state management**
  - Choose appropriate state management for your app's complexity
  - Avoid prop drilling through many component levels
  - Keep state as close as possible to where it's needed

- **Ensure accessibility**
  - Use semantic HTML elements
  - Add proper ARIA attributes for complex elements
  - Ensure keyboard navigability
  - Maintain sufficient color contrast

## Security vulnerabilities to prevent

- **SQL/NoSQL injection**
  - Never concatenate user input directly into queries
  - Use parameterized queries or ORM methods

- **Cross-site scripting (XSS)**
  - Sanitize user input before displaying it
  - Use frameworks' built-in protection mechanisms

- **Cross-site request forgery (CSRF)**
  - Implement anti-CSRF tokens
  - Validate request origins

- **Broken authentication**
  - Implement proper session management
  - Use secure password hashing
  - Enforce strong password policies